# Download file from API and unzip to storage account

```sh
- We have to do API call which will download the zip file from the given URL.
- Then we have to unzip the downloaded zip file and upload the unzipped files to the storag account.
- Create data pipeline for this..

API call ----> unzip and copy into storage --> storage account
```

## Create on Storage Account

- Go to azure portal --> Click on storage account --> Click on create storage account.
- Choose the resource group (RG_VNet01)
- Storage account name (name shouldberare - same account) 
- Region (US) East US
- Primary service ()
- Performance (Standard)
- Click on create.. (finish Deployment is in progress)

===

- Goto to resource
- Open Data storage (under the overview) --> Containers.
- Create a new container (name - output)
- Create

===

(Note: same data factory use hear - nameshouldberare01)
So directly goto 'lanch studio'

===

- Lanch studio --> it will direct to another tap
- Goto author (pencile symble)
- Click Pipeline ... (New pipeline under the home symble)
- Activities --> searh copy (copy data) Move and transfor --> Copy data (like drag to blak page) once it drage some settings will opend
- Goto Source (General - Source - Sink - Mapping - Settings)

### Source

- Select Source type (HTTP)
- Source dataset --> + New (HTTP), click continew
- Select format --> Binary (file is zip so we select binary), click continew.
- Set properties --> Name (Binary) and Linked service (+ new)
- New linked service
- Name (HttpServer)
- Connect via integration runtime (AutoResolveIntegrationRuntime)
- Base URL ()
- Authentication (Anonymous)
- Test connection (OK) than
- Create

===

- Now in source --> source dataset --> OPEN (once we click open on that compression type we have to give)

- Compression type (ZipDeflate(.zip))
- Compression level (Optimal)

===

### Sink

- Select Sink type (Azure Blob Storage)
- Sink dataset --> + New (Select the Azure Blob Storage), click continew
- Click to continew - than select to Binary file-->  continew

===

- Set properties --> Name (Binary) and Linked service* (+ new)

===

- New linked service
- Name (AzureBlobStorage1)
- Connect via integration runtime (AutoResolveIntegrationRuntime)
- Authentication type (Account key)
- Account selection method
- From Azure subscription (Azure subscription)
- Storage account name (nameshouldberare)

===

- Create (our link services is ready now)

- Now the file path where you want to save

### Set properties

- Name (Binary)
- Linked service (AzureBlobStorage1)
- File path (data/reqres.csv) (output)
- Import schema (none)

===

- Inthe file path end there is a broser box click on theat (we need to copy file and select the output)
- OK (Now the sink is ready)

===

- In Sink --> sink detaset --> OPEN
- Compression type (None donot select anything)

### Source

- Preserve zip file name as folder (untick)

- Debug....

====