# Copy data from rest-API to ADLS/BLOB storage

```sh
- We are getting data form rest API and we want to transfer and store this data in storage account.
- Create adf pipeline which will read the data form rest API and move into storage account.
- Capture only the required and important field into ADLS.

Rest-APT ---> copy data ---> Storage (ADLS)
```

## Create on Storage Account

- Go to azure portal --> Click on storage account --> Click on create storage account.
- Choose the resource group (RG_VNet01)
- Storage account name (name should be rare)
- Region (US) East US
- Primary service ()
- Performance (Standard)
- Click on create.. (finish Deployment is in progress)

- Goto to resource
- Open Data storage --> Containers.
- Create a new container (name - input)
- Create

- find the URL on web (https://reqres.in/api/users)

## Create a pipeline in azure data factory (ADF)

- Go to azure portal --> Click on data factory --> Click on create data factory.
- Subscription (Azure subscription 1)
- Resource group (RG_VNet01)
- Name (nameshouldberare01)
- Location (US) East US
- Version (V2)
- Click on create
===

- Lanch studio --> it will direct to another tap
- Goto author (pencile symble)
- Click Pipeline ... (New pipeline under the home symble)
- Activities --> Move and transform --> Copy data (like drag to blak page) once it drage some settings will opend
- Goto Source (General - Source - Sink - Mapping - Settings)

### Source

- Select Source type (REST API)
- Source dataset --> + New (Select the Rest), click continew
- Set properties --> Name (RestResource1) and Linked service* (+ new)
- New linked service
- Name (RestService1)
- Connect via integration runtime (AutoResolveIntegrationRuntime)
- Base URL (https://reqres.in/api/users)
- Authentication (Anonymous)
- Test connection (OK) than
- Create
===

- Now check the Preview data (data will be a formate or not)

### Sink

- Select Sink type (Azure Blob Storage)
- Sink dataset --> + New (Select the Azure Blob Storage), click continew
- Click to continew - than select to CSV file (DelimitedText) continew
- Set properties --> Name (DelimitedText1) and Linked service* (+ new)
===

- New linked service
- Name (AzureBlobStorage1)
- Connect via integration runtime (AutoResolveIntegrationRuntime)
- Authentication type (Account key)
- Account selection method
- From Azure subscription (Azure subscription 1 (132747af-dc55-457d-883d-564deb1ac715))
- Storage account name (nameshouldberare)
===

- Create (our link services is ready now)

### Set properties

- Name (DelimitedText1)
- Linked service (AzureBlobStorage1)
- File path (data/reqres.csv)
- Import schema (none)
===

- Inthe file path end there is a broser box click on theat (we need to copy file and select the input)
- OK (Now the sink is ready)

### Mapping

- Import schemas
- we dont want some thigs under the (page, per_page, tatal, tatal_pager and support), and we just need (id, email, first_name, lase_name, avatar) so we delete noneed things
===

- Collection reference ($['data'])
- Map complex values to string (not select)
- Now we select some source type like (id = int64, email = string, first_neme = String)
===

- Debug Now

- Once debug completed (wait for the completed)
===

- Goto Storage account page --> select your storage --> data storage --> containers --> input
- Select the container (data)(data_390f8510-974f-446a-a8db-609874d7d63b_468b933)
- you want see the date.. goto Edit Now you can able to see the data..
===
